{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Automated Question Answering System .ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPfO3AmPvyScA2Vbjt4OzZS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AshwinCarvalho999/AutAutomated-Question-Answering-System/blob/main/Automated_Question_Answering_System_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OTfSQs9XmDWD"
      },
      "source": [
        "**Automatic Question Answering System**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0K_GzNel-S7"
      },
      "source": [
        "**Our aim is to find the correct answer for asked question.The user input can be slightly different from the based question**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KAOzylychW0t"
      },
      "source": [
        "import string\n",
        "import nltk\n",
        "from nltk.corpus import wordnet\n",
        "import sys\n",
        "import re\n",
        "try:\n",
        "    from Levenshtein.StringMatcher import StringMatcher as SequenceMatcher\n",
        "except ImportError:\n",
        "    from difflib import SequenceMatcher"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2g6ht97sl1Wz"
      },
      "source": [
        "training_questions1 = []\n",
        "training_answers1 = []\n",
        "testing_questions1 = []\n",
        "testing_answers1 = []\n",
        "count = 0\n",
        "with open('training_dataset.txt') as training_dataset1: \n",
        "    for sentence in training_dataset1:\n",
        "        if count%2 == 0:\n",
        "            training_questions1.append(sentence[:-1])\n",
        "        else:\n",
        "            training_answers1.append(sentence[:-1])\n",
        "        count +=1\n",
        "\n",
        "count = 0\n",
        "with open('test_dataset.txt') as testing_dataset:\n",
        "#for question in training_dataset1:\n",
        "    for sentence in testing_dataset:\n",
        "        if count%2 == 0:\n",
        "            testing_questions1.append(sentence[:-1])\n",
        "        else:\n",
        "            if sentence[-1] == '\\n':\n",
        "                    testing_answers1.append(sentence[:-1].lower())\n",
        "            else:\n",
        "                testing_answers1.append(sentence)\n",
        "        count+=1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rk5uPAvOl6QH"
      },
      "source": [
        "**Lets first extract the questions from the training dataset and take a look at them.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y0nYV0Kql1S_"
      },
      "source": [
        "testing_answers1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9pd65O0rmvF5"
      },
      "source": [
        "['hi',\n",
        "\n",
        "'nothing much',\n",
        "\n",
        "'greetings',\n",
        "\n",
        "\"i'm doing good\",\n",
        "\n",
        "\"i'm nameless\",\n",
        "\n",
        "'anything that you want',\n",
        "\n",
        "'the date specified on your reservation',\n",
        "\n",
        "'whenever you want to',\n",
        "\n",
        " 'i am your little assistant',\n",
        "\n",
        " 'hi',\n",
        "\n",
        " \"i don't have an address\",\n",
        "\n",
        " \"i don't have phone number either\",\n",
        "\n",
        " \"you can't call me, you can only talk to me here\",\n",
        "\n",
        " \"you can't call me, you can only talk to me here\",\n",
        "\n",
        " 'sure, what is their phone number',\n",
        "\n",
        " \"sure, i'm getting you someone to talk with you\",\n",
        "\n",
        " 'take the metro and stop at the terminus, we are 5mins away from there',\n",
        "\n",
        " 'you need to take the tgv then at paris train station you take the subway',\n",
        "\n",
        " 'you need to take the tgv then at paris train station you take the subway',\n",
        "\n",
        " 'none',\n",
        "\n",
        " 'you need to take the periph and drive for 80mins',\n",
        "\n",
        " 'you need to take the periph and drive for 40mins',\n",
        " \n",
        " 'sure, about what',\n",
        "\n",
        " 'sure, about what',\n",
        "\n",
        " 'well, depends on how urgent you need him',\n",
        "\n",
        " 'can you come to the clinic',\n",
        "\n",
        " 'sure, for what',\n",
        "\n",
        " 'well, please fill the form below']\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zned6NvEnixk"
      },
      "source": [
        "First let us remove the stopwords from the dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cW8ykq1ns7D"
      },
      "source": [
        "**STOPWORDS**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4rmAS2g8n1Y3"
      },
      "source": [
        "The words that occur most frequently in all the documents, they carry no meaning to the document like 'is','are','the', 'of' etc.\n",
        "Lets make a list of stopwords"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hghwfitil1QA"
      },
      "source": [
        "#stopwords = nltk.corpus.stopwords.words('english')\n",
        "stopwords = ['of','to','is','the','are','at','i','if']\n",
        "stopwords.extend(string.punctuation)\n",
        "stopwords.append('')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GHjvvkZ8oTTV"
      },
      "source": [
        "**Now, we can do few things here,**\n",
        "\n",
        "1.   Normal String matching\n",
        "2.   Token matching\n",
        "3.   Lemma matching\n",
        "4.   Partial String Matching\n",
        "5.   We can apply LSA here\n",
        "6.   We can use Jackard Coefficient"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0EKcaAUoGwU"
      },
      "source": [
        "**Now we are going to follow these steps:-**\n",
        "\n",
        "First break the sentence in tokens.\n",
        "Then we will lemmatize the tokens to get its base word.\n",
        "Remove the tokens that are stopwords.\n",
        "Now we will find the jackard coefficient of the sentences.\n",
        "If answer not found then we will use partial_noun_lemma_match\n",
        "Even if answer is not found we use levenshtein_distance\n",
        "If answer not found upto step 6, then we return 'None'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FoR0mlMwn4-O"
      },
      "source": [
        "def get_pos(pos_tag):\n",
        "    if pos_tag[1][0] == 'J':\n",
        "        return (pos_tag[0], wordnet.ADJ)\n",
        "    elif pos_tag[1][0] == 'V':\n",
        "        return (pos_tag[0], wordnet.VERB)\n",
        "    elif pos_tag[1][0] == 'R':\n",
        "        return (pos_tag[0], wordnet.ADV)\n",
        "    elif pos_tag[1][0] == 'N':\n",
        "        return (pos_tag[0], wordnet.NOUN)\n",
        "    else:\n",
        "        return (pos_tag[0], wordnet.NOUN)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbC-p8mJqS00"
      },
      "source": [
        "Some words will be creating problems. So, remove those words. If there was more time then I might have used autocorrection code to correct any misspelled word (But i preffered not to)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IamUT8Gln472"
      },
      "source": [
        "def correct_words(sentence):\n",
        "    output = []\n",
        "    for word in sentence.lower().split(' '):\n",
        "        if word == \"i'm\":\n",
        "            output.append('i am')\n",
        "        elif word == \"what's\":\n",
        "            output.append('what is')\n",
        "        elif word == \"where's\":\n",
        "            output.append('what is')\n",
        "        elif word == 'check-out':\n",
        "            output.append('check out')\n",
        "        elif word == 'check-in':\n",
        "            output.append('check in')\n",
        "        elif word == 'checkin':\n",
        "            output.append('check in')\n",
        "        elif word == 'checkout':\n",
        "            output.append('check out')\n",
        "        elif word == 'wi-fi' or word == 'wifi':\n",
        "            output.append('wi fi')\n",
        "        elif word == 'yours' or word == 'yourself' or word == 'yours' or word == 'yourselves':\n",
        "            output.append('your')\n",
        "        elif word == 'ours' or word == 'ourself' or word == 'ourselves' or word == 'me' or word == 'my' or word == 'myself':\n",
        "            output.append('i')\n",
        "        elif word in stopwords:\n",
        "            pass\n",
        "        else:\n",
        "            word = re.sub('[^a-zA-Z]',' ',word).split(' ')[0]\n",
        "            output.append(word)\n",
        "    return ' '.join(output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vn4GR61In44-"
      },
      "source": [
        "lemmatizer = nltk.stem.wordnet.WordNetLemmatizer()\n",
        "def data_cleaning(query_sentence,data_sentence):\n",
        "    pos_a = map(get_pos, nltk.pos_tag(nltk.word_tokenize(query_sentence)))\n",
        "    pos_b = map(get_pos, nltk.pos_tag(nltk.word_tokenize(data_sentence)))\n",
        "    lemmae_a = [lemmatizer.lemmatize(token.lower().strip(string.punctuation), pos) for token, pos in pos_a \\\n",
        "                    if token.lower().strip(string.punctuation) not in stopwords]\n",
        "    lemmae_b = [lemmatizer.lemmatize(token.lower().strip(string.punctuation), pos) for token, pos in pos_b \\\n",
        "                    if token.lower().strip(string.punctuation) not in stopwords]\n",
        "    #print(lemmae_a)\n",
        "    #print(lemmae_b)\n",
        "    ratio = len(set(lemmae_a).intersection(lemmae_b)) / float(len(set(lemmae_a).union(lemmae_b)))\n",
        "    return (ratio > 0.60,ratio)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lzgZ-ZZHqgbE"
      },
      "source": [
        "I tried finding accuracy upto this point and found that sentence like\n",
        "'how to get there from switzerland'\n",
        "are not matched. So, if the sentence is not matched in previous stage, then we match the sentence via partial_noun_lemma_match."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lPn9LQ0xn4z4"
      },
      "source": [
        "def partial_noun_lemma_match(a, b):\n",
        "    \"\"\"Check if a and b are matches.\"\"\"\n",
        "    pos_a = map(get_pos, nltk.pos_tag(nltk.word_tokenize(a)))\n",
        "    pos_b = map(get_pos, nltk.pos_tag(nltk.word_tokenize(b)))\n",
        "    lemmae_a = [lemmatizer.lemmatize(token.lower().strip(string.punctuation), pos) for token, pos in pos_a \\\n",
        "                    if pos == wordnet.NOUN and token.lower().strip(string.punctuation) not in stopwords]\n",
        "    lemmae_b = [lemmatizer.lemmatize(token.lower().strip(string.punctuation), pos) for token, pos in pos_b \\\n",
        "                    if pos == wordnet.NOUN and token.lower().strip(string.punctuation) not in stopwords]\n",
        "    #if a == 'how to get there from switzerland':\n",
        "    #    print(lemmae_a)\n",
        "    #    print(lemmae_b)\n",
        "    # Calculate Jaccard similarity\n",
        "    try:\n",
        "        ratio = len(set(lemmae_a).intersection(lemmae_b)) / float(len(set(lemmae_a).union(lemmae_b)))\n",
        "        return (ratio >= 0.60,ratio)\n",
        "    except:\n",
        "        return None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3HDqqaVLqqVG"
      },
      "source": [
        "I want to increase the results of second dataset. So, I explored the dataset and found that Levenshtein_distance would do the task.\n",
        "\n",
        "**Levenshtein_distance**\n",
        "\n",
        "Levenshtein distance between two words is the minimum number of single-character edits.\n",
        "\n",
        "For example(taken from Wikipedia), the Levenshtein distance between \"kitten\" and \"sitting\" is 3, since the following three edits change one into the other, and there is no way to do it with fewer than three edits:\n",
        "kitten → sitten (substitution of \"s\" for \"k\")\n",
        "sitten → sittin (substitution of \"i\" for \"e\")\n",
        "sittin → sitting (insertion of \"g\" at the end)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_gRvZprjqwPd"
      },
      "source": [
        "def levenshtein_distance(statement, other_statement):\n",
        "    '''\n",
        "    statement1 = statement.split(' ')\n",
        "    statement2 = [w for w in statement1 if w not in stopwords]\n",
        "    statement = ' '.join(statement2)\n",
        "    \n",
        "    other_statement1 = other_statement.split(' ')\n",
        "    other_statement2 = [w for w in other_statement1 if w not in stopwords]\n",
        "    other_statement = ' '.join(other_statement2)\n",
        "    '''\n",
        "    #if statement == 'do u have wifi in the room':\n",
        "    #    print('shout')\n",
        "    statement = correct_words(statement)\n",
        "    other_statement = correct_words(other_statement)    \n",
        "    PYTHON = sys.version_info[0]\n",
        "    if not statement or not other_statement:\n",
        "        return 0\n",
        "\n",
        "    if PYTHON < 3:\n",
        "        statement_text = unicode(statement.lower())\n",
        "        other_statement_text = unicode(other_statement.lower())\n",
        "    else:\n",
        "        statement_text = str(statement.lower())\n",
        "        other_statement_text = str(other_statement.lower())\n",
        "\n",
        "    similarity = SequenceMatcher(None,statement_text,other_statement_text)\n",
        "    percent = int(round(100 * similarity.ratio())) / 100.0\n",
        "    return percent"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Siacks0Jq_3K"
      },
      "source": [
        "Combining Everything"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BDx6seI9qw4W"
      },
      "source": [
        "import string\n",
        "import nltk\n",
        "from nltk.corpus import wordnet\n",
        "import sys\n",
        "\n",
        "try:\n",
        "    from Levenshtein.StringMatcher import StringMatcher as SequenceMatcher\n",
        "except ImportError:\n",
        "    from difflib import SequenceMatcher\n",
        "    \n",
        "def file_read(train_file,test_file):\n",
        "    '''\n",
        "    Read the file\n",
        "    train_file :- Training_File\n",
        "    test_file :- Testing_File\n",
        "    return :- training_question,training_answers,testing_questions,testing_answers\n",
        "    '''\n",
        "    count = 0\n",
        "    training_questions1 = []\n",
        "    training_answers1 = []\n",
        "    testing_questions1 = []\n",
        "    testing_answers1 = []\n",
        "    with open(train_file) as training_dataset1: \n",
        "        for sentence in training_dataset1:\n",
        "            if count%2 == 0:\n",
        "                training_questions1.append(sentence[:-1].lower())\n",
        "            else:\n",
        "                if sentence[-1] == '\\n':\n",
        "                    training_answers1.append(sentence[:-1].lower())\n",
        "            count +=1\n",
        "\n",
        "    count = 0\n",
        "    with open(test_file) as testing_dataset:\n",
        "    #for question in training_dataset1:\n",
        "        for sentence in testing_dataset:\n",
        "            #print(sentence)\n",
        "            if count%2 == 0:\n",
        "                testing_questions1.append(sentence[:-1].lower())\n",
        "            else:\n",
        "                if sentence[-1] == '\\n':\n",
        "                    testing_answers1.append(sentence[:-1].lower())\n",
        "            count+=1\n",
        "            \n",
        "    return training_questions1,training_answers1,testing_questions1,testing_answers1\n",
        "\n",
        "def argmax(array):\n",
        "    index = 0\n",
        "    maximum = -999\n",
        "    for i in range(len(array)):\n",
        "        if array[i] > maximum:\n",
        "            maximum = array[i]\n",
        "            index = i\n",
        "    return index\n",
        "\n",
        "def get_pos(pos_tag):\n",
        "    '''\n",
        "    return:- tag of word\n",
        "    '''\n",
        "    if pos_tag[1][0] == 'J':\n",
        "        return (pos_tag[0], wordnet.ADJ)\n",
        "    elif pos_tag[1][0] == 'V':\n",
        "        return (pos_tag[0], wordnet.VERB)\n",
        "    elif pos_tag[1][0] == 'R':\n",
        "        return (pos_tag[0], wordnet.ADV)\n",
        "    elif pos_tag[1][0] == 'N':\n",
        "        return (pos_tag[0], wordnet.NOUN)\n",
        "    else:\n",
        "        return (pos_tag[0], wordnet.NOUN)\n",
        "        \n",
        "def correct_words(sentence):\n",
        "    '''\n",
        "    Change words to desired words\n",
        "    sentence :- input sentence\n",
        "    return :- correct_sentence\n",
        "    '''\n",
        "    output = []\n",
        "    for word in sentence.lower().split(' '):\n",
        "        if word == \"i'm\":\n",
        "            output.append('i am')\n",
        "        elif word == \"what's\":\n",
        "            output.append('what is')\n",
        "        elif word == \"where's\":\n",
        "            output.append('what is')\n",
        "        elif word == 'check-out':\n",
        "            output.append('check out')\n",
        "        elif word == 'check-in':\n",
        "            output.append('check in')\n",
        "        elif word == 'checkin':\n",
        "            output.append('check in')\n",
        "        elif word == 'checkout':\n",
        "            output.append('check out')\n",
        "        elif word == 'wi-fi' or word == 'wifi':\n",
        "            output.append('wi fi')\n",
        "        elif word == 'yours' or word == 'yourself' or word == 'yours' or word == 'yourselves':\n",
        "            output.append('your')\n",
        "        elif word == 'ours' or word == 'ourself' or word == 'ourselves' or word == 'me' or word == 'my' or word == 'myself':\n",
        "            output.append('i')\n",
        "        elif word in stopwords:\n",
        "            pass\n",
        "        else:\n",
        "            word = re.sub('[^a-zA-Z]',' ',word).split(' ')[0]\n",
        "            output.append(word)\n",
        "    return ' '.join(output)\n",
        "\n",
        "lemmatizer = nltk.stem.wordnet.WordNetLemmatizer()\n",
        "\n",
        "def data_cleaning(query_sentence,data_sentence):\n",
        "    '''\n",
        "    This function finds the Jaccard Coefficient of query_sentence and data_sentence\n",
        "    query_sentence:- question user asked\n",
        "    data_sentence:- question in training_data \n",
        "    return:- (ratio > 0.6,ratio)\n",
        "    '''\n",
        "    pos_a = map(get_pos, nltk.pos_tag(nltk.word_tokenize(query_sentence)))\n",
        "    pos_b = map(get_pos, nltk.pos_tag(nltk.word_tokenize(data_sentence)))\n",
        "    lemmae_a = [lemmatizer.lemmatize(token.lower().strip(string.punctuation), pos) for token, pos in pos_a \\\n",
        "                    if token.lower().strip(string.punctuation) not in stopwords]\n",
        "    lemmae_b = [lemmatizer.lemmatize(token.lower().strip(string.punctuation), pos) for token, pos in pos_b \\\n",
        "                    if token.lower().strip(string.punctuation) not in stopwords]\n",
        "    #print(lemmae_a)\n",
        "    #print(lemmae_b)\n",
        "    ratio = len(set(lemmae_a).intersection(lemmae_b)) / float(len(set(lemmae_a).union(lemmae_b)))\n",
        "    return (ratio > 0.60,ratio)\n",
        "    \n",
        "    \n",
        "def levenshtein_distance(statement, other_statement):\n",
        "    '''\n",
        "    statement1 = statement.split(' ')\n",
        "    statement2 = [w for w in statement1 if w not in stopwords]\n",
        "    statement = ' '.join(statement2)\n",
        "    \n",
        "    other_statement1 = other_statement.split(' ')\n",
        "    other_statement2 = [w for w in other_statement1 if w not in stopwords]\n",
        "    other_statement = ' '.join(other_statement2)\n",
        "    '''\n",
        "    #if statement == 'do u have wifi in the room':\n",
        "    #    print('shout')\n",
        "    statement = correct_words(statement)\n",
        "    other_statement = correct_words(other_statement)    \n",
        "    PYTHON = sys.version_info[0]\n",
        "    if not statement or not other_statement:\n",
        "        return 0\n",
        "\n",
        "    if PYTHON < 3:\n",
        "        statement_text = unicode(statement.lower())\n",
        "        other_statement_text = unicode(other_statement.lower())\n",
        "    else:\n",
        "        statement_text = str(statement.lower())\n",
        "        other_statement_text = str(other_statement.lower())\n",
        "\n",
        "    similarity = SequenceMatcher(None,statement_text,other_statement_text)\n",
        "    percent = int(round(100 * similarity.ratio())) / 100.0\n",
        "    return percent\n",
        "\n",
        "\n",
        "def partial_noun_lemma_match(a, b):\n",
        "    '''\n",
        "    This function finds the Jaccard Coefficient of query_sentence and data_sentence (Noun words)\n",
        "    query_sentence:- question user asked\n",
        "    data_sentence:- question in training_data \n",
        "    return:- (ratio > 0.6,ratio)\n",
        "    '''\n",
        "    pos_a = map(get_pos, nltk.pos_tag(nltk.word_tokenize(a)))\n",
        "    pos_b = map(get_pos, nltk.pos_tag(nltk.word_tokenize(b)))\n",
        "    lemmae_a = [lemmatizer.lemmatize(token.lower().strip(string.punctuation), pos) for token, pos in pos_a \\\n",
        "                    if pos == wordnet.NOUN and token.lower().strip(string.punctuation) not in stopwords]\n",
        "    lemmae_b = [lemmatizer.lemmatize(token.lower().strip(string.punctuation), pos) for token, pos in pos_b \\\n",
        "                    if pos == wordnet.NOUN and token.lower().strip(string.punctuation) not in stopwords]\n",
        "    #if a == 'how to get there from switzerland':\n",
        "    #    print(lemmae_a)\n",
        "    #    print(lemmae_b)\n",
        "    # Calculate Jaccard similarity\n",
        "    try:\n",
        "        ratio = len(set(lemmae_a).intersection(lemmae_b)) / float(len(set(lemmae_a).union(lemmae_b)))\n",
        "        return (ratio >= 0.60,ratio)\n",
        "    except:\n",
        "        return None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zn3hJRTrqw1Y"
      },
      "source": [
        "\n",
        "def head_function(training_questions,training_answers,testing_questions,testing_answers):\n",
        "    '''\n",
        "    This function calls all the functions and finds the count of queries matched.\n",
        "    First calls data_cleaning. If answer not found then it calls partial_noun_lemma_match even if answer not found\n",
        "    it calls levenshtein_distance. If answer is not found upto this stage , it returns None.\n",
        "    return count of answers correctly identified.\n",
        "    '''\n",
        "    count = 0\n",
        "    output = []\n",
        "    ratio = []\n",
        "    for i,target_sentence in enumerate(testing_questions):\n",
        "        target_sentence = target_sentence.lower()\n",
        "        print('----------------')\n",
        "        #print(target_sentence)\n",
        "        ratios = []\n",
        "        answers = []\n",
        "        answer = ''\n",
        "        for j,sentence in enumerate(training_questions):\n",
        "            sentence = sentence.lower()\n",
        "            output.append((data_cleaning(target_sentence, sentence),j))\n",
        "            ratios.append(data_cleaning(target_sentence, sentence)[1])\n",
        "            answers.append(testing_answers[j])\n",
        "\n",
        "        if(ratios[argmax(ratios)] > 0.6):\n",
        "            answer = training_answers[argmax(ratios)]\n",
        "            print(target_sentence,answer,testing_answers[i])\n",
        "            #count +=1\n",
        "        else:\n",
        "            output = output[:-1]\n",
        "            ratios = ratios[:-1]\n",
        "            answers = answers[:-1]\n",
        "            output.append((partial_noun_lemma_match(target_sentence, sentence),j))\n",
        "            #print(output[j][0][1])\n",
        "            ratios.append(partial_noun_lemma_match(target_sentence, sentence)[1])\n",
        "            answers.append(testing_answers[j])\n",
        "            if(ratios[argmax(ratios)] >= 0.6):\n",
        "                answer = training_answers[argmax(ratios)]\n",
        "                print(target_sentence,answer,testing_answers[i])\n",
        "            else:\n",
        "                output = output[:-1]\n",
        "                ratios = ratios[:-1]\n",
        "                answers = answers[:-1]\n",
        "                distance = []\n",
        "                for j,sentence in enumerate(training_questions):\n",
        "                    distance.append(levenshtein_distance(target_sentence,sentence))\n",
        "                if(max(distance) >= 0.4):\n",
        "                    maximum = -999\n",
        "                    index = 0\n",
        "                    for j,dist in enumerate(distance):\n",
        "                        if dist> maximum:\n",
        "                            maximum = dist\n",
        "                            index = j\n",
        "                    answer = training_answers[index] \n",
        "                    print(target_sentence,answer,testing_answers[i])\n",
        "                else:\n",
        "                    #print('None')\n",
        "                    answer = 'None'\n",
        "        if(answer == testing_answers[i]):\n",
        "                count +=1\n",
        "                #print('count')\n",
        "        #print(target_sentence)\n",
        "        #print('----------------')\n",
        "    return count"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XXpYgg6irb4g"
      },
      "source": [
        "stopwords = ['of','to','is','the','are','at','if','am']\n",
        "stopwords.extend(string.punctuation)\n",
        "stopwords.append('')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-REHef7IrbvK"
      },
      "source": [
        "print('Output is printed in following form:-')\n",
        "print('(asked_question,our_answer,real_answer)')\n",
        "training_questions,training_answers,testing_questions,testing_answers = \\\n",
        "                                file_read('training_dataset.txt','test_dataset.txt')\n",
        "count_of_correct_answers = head_function(training_questions,training_answers,testing_questions,testing_answers)\n",
        "print()\n",
        "print('**************************************************************************')\n",
        "print('Number of Correct answers:-',count_of_correct_answers)\n",
        "print('**************************************************************************')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N43r0v8Rsb9j"
      },
      "source": [
        "('Number of Correct answers:-', 32)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oAfze8UVqwt2"
      },
      "source": [
        "print('Output is printed in following form:-')\n",
        "print('(asked_question,our_answer,real_answer)')\n",
        "training_questions,training_answers,testing_questions,testing_answers = \\\n",
        "                                file_read('training_dataset_2.txt','test-data.txt')\n",
        "count_of_correct_answers = head_function(training_questions,training_answers,testing_questions,testing_answers)\n",
        "print('**************************************************************************')\n",
        "print('Number of Correct Answers:-',count_of_correct_answers)\n",
        "print('**************************************************************************')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MEcvYq1QsfAp"
      },
      "source": [
        "('Number of Correct Answers:-', 86)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LtZRCuGCsOz6"
      },
      "source": [
        "\n",
        "**Correctly Identified Answers in set 1**:- $\\frac{32}{33}$\n",
        "\n",
        "**Correctly Identified Answers in set 2**:- $\\frac{86}{93}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yo1YCqKmsQr4"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8U1sBb6WsQup"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}